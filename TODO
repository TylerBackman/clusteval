Fix the following simulations:
	Both Ramey methods are breaking on these data sets.
		Wen - (p = 9)
		Bhattacharjee

Update the working data sets to q = 200, B = 100, num_clusters = 3:7, and
	cl_methods <- c("hierarchical", "kmeans", "pam", "model", "diana")
	similarity_methods <- c("rand", "jaccard", "adjustedrand")
Run them! (Kodiak?)


It would be useful to consider an external validation approach here as well.
	Rather than using a clustering method to get the truth, use the true, external
	labels to see how well the method could match it.

Consider using 'clara' as a clustering method. To quote Hennig's documentation,
	"PAM is better, CLARA is faster"
	If this is true, then CLARA may yield results that we can criticize.

Data Sets with 'issues' that we should come back to:
	Bhattacharjee (the 'harvard' data set in 'lungExpression')

Add bootstrap_type argument to clustomit_landon
	Should be in: c("nonparametric, parametric")
	If nonparametric, use what we already have. with_replacement only affects this.
	If parametric, use the method in section 3.3 in Hennig
		An additional argument should be added to clustomit_landon that is a RNG function that matches the
			criteria in 'boot'
		If it is NULL, we use the one in Hennig.
		This should allow additional support for other types of parametric bootstrapping/jittering
			such as the one we used in the original paper.

Once I figure out what a good value for B (the bootstrap replicates) is, update the default values
	for each of the clustomit functions




Add more documentation to clustomit_boot
Add documentation to helper functions.

Make sure the WARNINGS being thrown by the similarity functions is not critical.
	Fix it, if possible.
	
	
